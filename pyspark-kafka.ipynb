{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Assignment 4: PySpark Structured Streaming Using Kafka Source**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-kafka-streaming\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0\"). \\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ==== Q2 =============================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.1**: Create a streaming DataFrame in Spark that reads data from a Kafka topic named \"topic_test\" and starts processing from the beginning of the topic's log using the earliest available offset. Consult the [documentation](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html#reading-data-from-kafka). [10 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer to 2.1\n",
    "df_streamed_raw = (spark\n",
    "  .readStream\n",
    "  # Add your code here\n",
    "  .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# convert byte stream to string\n",
    "df_streamed_kv = (df_streamed_raw\n",
    "    .withColumn(\"key\", df_streamed_raw[\"key\"].cast(StringType()))\n",
    "    .withColumn(\"value\", df_streamed_raw[\"value\"].cast(StringType())))\n",
    "\n",
    "test_query = (df_streamed_kv \n",
    "              .writeStream \\\n",
    "              .format(\"memory\") # output to memory \\\n",
    "              .outputMode(\"update\") # only write updated rows to the sink \\\n",
    "              .queryName(\"test_query_table\")  # Name of the in memory table \\\n",
    "              .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If all goes well, the following cell should display a table populated with values being streamed from you Kafka producer. NOTE: If you recently ran the producer, it may take a while before the table is populated. Keep rerunning the cell to check for updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from test_query_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import StructType, StructField, BooleanType, LongType, IntegerType\n",
    "\n",
    "event_schema = StructType([\n",
    "    StructField(\"ip_address\", StringType()),\n",
    "    StructField(\"date_time\", StringType()),\n",
    "    StructField(\"request_type\", StringType()),\n",
    "    StructField(\"request_arg\", StringType()),\n",
    "    StructField(\"status_code\", StringType()),\n",
    "    StructField(\"response_size\", StringType()),\n",
    "    StructField(\"referrer\", StringType()),\n",
    "    StructField(\"user_agent\", StringType())\n",
    "])\n",
    "\n",
    "# Parse the events from JSON format\n",
    "df_parsed = (df_streamed_kv\n",
    "           # Sets schema for event data\n",
    "           .withColumn(\"value\", from_json(\"value\", event_schema))\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_formatted = (df_parsed.select(\n",
    "    col(\"key\").alias(\"event_key\")\n",
    "    ,col(\"topic\").alias(\"event_topic\")\n",
    "    ,col(\"timestamp\").alias(\"event_timestamp\")\n",
    "    ,col(\"value.ip_address\").alias(\"ip_address\")\n",
    "    ,col(\"value.date_time\").alias(\"date_time\")\n",
    "    ,col(\"value.request_type\").alias(\"request_type\")\n",
    "    ,col(\"value.request_arg\").alias(\"request_arg\")\n",
    "    ,col(\"value.status_code\").alias(\"status_code\")\n",
    "    ,col(\"value.response_size\").cast(IntegerType()).alias(\"response_size\")\n",
    "    ,col(\"value.referrer\").alias(\"referrer\")\n",
    "    ,col(\"value.user_agent\").alias(\"user_agent\")\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q2.2**: Read up on output sinks in the documentation. Select the appropriate Sink and write the streaming data received from the Kafka topic \"topic_test\" to an in-memory table named \"access_log_table\" and configure the output mode to append the table with only the changed rows. [5 points]spark.sql(\"select count(1) from query_sma\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to 2.2\n",
    "query = (df_formatted\n",
    " .writeStream\n",
    " # Add your code here\n",
    " .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given\n",
    "for s in spark.streams.active:\n",
    "    print(f\"ID:{s.id} | NAME:{s.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ==== Q3 =============================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q3.1**: Write code to continuously count the number of rows in access_log_table while the streaming query is active. Update this output every 5 seconds. HINT: You can use the clear_output function from the IPython.display module to clear the output console. [15 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "try:\n",
    "    #=========== 3.1 =============================\n",
    "    pass\n",
    "    #=========== 3.1 =============================\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "#     test_query.stop()\n",
    "    print(\"stream process interrupted\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q3.2**: Count the number of requests per type (i.e GET, POST etc) so far. [15 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q3.3**: Compute the simple moving average of request size with a window size of the latest 10 requests. [20 Points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q3.4**: Find the sizes of the request that are more than one moving standard deviation above the moving average [20 Points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
